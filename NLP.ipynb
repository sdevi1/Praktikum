{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/andrey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#импортируем библиотеки\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#загрузим датасет\n",
    "df = pd.read_csv('/home/andrey/Загрузки/toxic_comments.csv')\n",
    "\n",
    "#посмотрим на датасет\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0.113188\n",
       "toxic    0.113188\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['toxic'] == 1].count() / df[df['toxic'] == 0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токсичных комментариев в выборке примерно 11%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные состоят из твитов на английском языке лемматизируем их с помощью spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#лемматизируем текст\n",
    "corpus = df['text'].values\n",
    "\n",
    "lemmed_text = []\n",
    "for i in range(len(corpus)):\n",
    "    text = nlp(corpus[i])\n",
    "    string = []\n",
    "    for token in text:\n",
    "        string.append(token.lemma_)\n",
    "    lemmed_text.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#избавимся от лишних символов с помощью регулярных выражений\n",
    "\n",
    "re_text = []\n",
    "\n",
    "for sen in range(0, len(lemmed_text)):\n",
    "    \n",
    "    document = re.sub(r'\\W', ' ', str(lemmed_text[sen]))\n",
    "    \n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    document = document.lower()\n",
    "    \n",
    "    re_text.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = pd.read_csv('/home/andrey/Загрузки/re_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_text = Z['re_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разделим выборку на train, test и valid\n",
    "y = df['toxic']\n",
    "X = re_text\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#применим downsampling для решения проблемы дисбаланса классов.\n",
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_downsampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])\n",
    "    target_downsampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])\n",
    "    \n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=12345)\n",
    "    \n",
    "    return features_downsampled, target_downsampled\n",
    "\n",
    "X_train, y_train = downsample(X_train, y_train, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#векторизируем текст\n",
    "tokenizer = nltk.casual.TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "count_vect = CountVectorizer(tokenizer=tokenizer.tokenize, stop_words=nltk_stopwords.words('english'),\n",
    "                             max_features = 5000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = count_vect.fit_transform(X_train).toarray()\n",
    "X_test = count_vect.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие гиперпараметры для модели логистической регрессии\n",
      "\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 12345, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97     35806\n",
      "           1       0.81      0.72      0.76      4087\n",
      "\n",
      "    accuracy                           0.95     39893\n",
      "   macro avg       0.89      0.85      0.87     39893\n",
      "weighted avg       0.95      0.95      0.95     39893\n",
      "\n",
      "\n",
      "F1 = 0.760274860624919\n",
      "CPU times: user 59.9 s, sys: 1.86 s, total: 1min 1s\n",
      "Wall time: 4min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logistic regression\n",
    "model = LogisticRegression(random_state=12345)\n",
    "\n",
    "param = {\"class_weight\" : ['balanced', None],\n",
    "         \"penalty\" : [\"l1\", \"l2\", \"elasticnet\", \"None\"],\n",
    "         \"solver\" : [\"liblinear\", \"lbfgs\"]\n",
    "        }\n",
    "\n",
    "model_rscv = RandomizedSearchCV(model, param_distributions = param, scoring = \"f1\",\n",
    "                             cv = 2, verbose = 0, random_state = 12345, n_jobs = 2, n_iter = 10)\n",
    "\n",
    "model_lr = model_rscv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Лучшие гиперпараметры для модели логистической регрессии\")\n",
    "print()\n",
    "print(model_lr.best_estimator_.get_params())\n",
    "\n",
    "pred = model_lr.predict(X_test)\n",
    "print()\n",
    "print(metrics.classification_report(y_test, pred))\n",
    "print()\n",
    "print(\"F1 = {}\".format(f1_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estim: 100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97     35806\n",
      "           1       0.78      0.73      0.75      4087\n",
      "\n",
      "    accuracy                           0.95     39893\n",
      "   macro avg       0.88      0.85      0.86     39893\n",
      "weighted avg       0.95      0.95      0.95     39893\n",
      " 0.7546261089987326\n",
      "CPU times: user 17min 4s, sys: 1.52 s, total: 17min 6s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#обучим модель случайного леса с перебором гиперпараметров\n",
    "\n",
    "for  i in range(100, 200, 100):\n",
    "    m = RandomForestClassifier(n_estimators = i, random_state=12345, n_jobs = -1)\n",
    "\n",
    "    m.fit(X_train, y_train)\n",
    "    predicts = m.predict(X_test)\n",
    "    print('estim:', i)\n",
    "    print(metrics.classification_report(y_test, predicts), metrics.f1_score(y_test, predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] n_estimators=500, min_child_weight=1, max_depth=12, gamma=0.0, eta=0.1, colsample_bytree=0.4 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=500, min_child_weight=1, max_depth=12, gamma=0.0, eta=0.1, colsample_bytree=0.4, score=0.795, total=  40.5s\n",
      "[CV] n_estimators=500, min_child_weight=1, max_depth=12, gamma=0.0, eta=0.1, colsample_bytree=0.4 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   40.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=500, min_child_weight=1, max_depth=12, gamma=0.0, eta=0.1, colsample_bytree=0.4, score=0.795, total=  40.0s\n",
      "[CV] n_estimators=500, min_child_weight=1, max_depth=12, gamma=0.0, eta=0.1, colsample_bytree=0.4 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=500, min_child_weight=1, max_depth=12, gamma=0.0, eta=0.1, colsample_bytree=0.4, score=0.796, total=  39.1s\n",
      "[CV] n_estimators=300, min_child_weight=3, max_depth=12, gamma=0.1, eta=0.05, colsample_bytree=0.7 \n",
      "[CV]  n_estimators=300, min_child_weight=3, max_depth=12, gamma=0.1, eta=0.05, colsample_bytree=0.7, score=0.747, total=  35.9s\n",
      "[CV] n_estimators=300, min_child_weight=3, max_depth=12, gamma=0.1, eta=0.05, colsample_bytree=0.7 \n",
      "[CV]  n_estimators=300, min_child_weight=3, max_depth=12, gamma=0.1, eta=0.05, colsample_bytree=0.7, score=0.754, total=  35.4s\n",
      "[CV] n_estimators=300, min_child_weight=3, max_depth=12, gamma=0.1, eta=0.05, colsample_bytree=0.7 \n",
      "[CV]  n_estimators=300, min_child_weight=3, max_depth=12, gamma=0.1, eta=0.05, colsample_bytree=0.7, score=0.748, total=  34.8s\n",
      "[CV] n_estimators=300, min_child_weight=1, max_depth=3, gamma=0.2, eta=0.2, colsample_bytree=0.3 \n",
      "[CV]  n_estimators=300, min_child_weight=1, max_depth=3, gamma=0.2, eta=0.2, colsample_bytree=0.3, score=0.747, total=  14.8s\n",
      "[CV] n_estimators=300, min_child_weight=1, max_depth=3, gamma=0.2, eta=0.2, colsample_bytree=0.3 \n",
      "[CV]  n_estimators=300, min_child_weight=1, max_depth=3, gamma=0.2, eta=0.2, colsample_bytree=0.3, score=0.750, total=  14.6s\n",
      "[CV] n_estimators=300, min_child_weight=1, max_depth=3, gamma=0.2, eta=0.2, colsample_bytree=0.3 \n",
      "[CV]  n_estimators=300, min_child_weight=1, max_depth=3, gamma=0.2, eta=0.2, colsample_bytree=0.3, score=0.750, total=  15.1s\n",
      "[CV] n_estimators=100, min_child_weight=3, max_depth=5, gamma=0.2, eta=0.3, colsample_bytree=0.3 \n",
      "[CV]  n_estimators=100, min_child_weight=3, max_depth=5, gamma=0.2, eta=0.3, colsample_bytree=0.3, score=0.741, total=   9.4s\n",
      "[CV] n_estimators=100, min_child_weight=3, max_depth=5, gamma=0.2, eta=0.3, colsample_bytree=0.3 \n",
      "[CV]  n_estimators=100, min_child_weight=3, max_depth=5, gamma=0.2, eta=0.3, colsample_bytree=0.3, score=0.745, total=   9.6s\n",
      "[CV] n_estimators=100, min_child_weight=3, max_depth=5, gamma=0.2, eta=0.3, colsample_bytree=0.3 \n",
      "[CV]  n_estimators=100, min_child_weight=3, max_depth=5, gamma=0.2, eta=0.3, colsample_bytree=0.3, score=0.745, total=   9.2s\n",
      "[CV] n_estimators=300, min_child_weight=5, max_depth=5, gamma=0.1, eta=0.05, colsample_bytree=0.7 \n",
      "[CV]  n_estimators=300, min_child_weight=5, max_depth=5, gamma=0.1, eta=0.05, colsample_bytree=0.7, score=0.693, total=  18.3s\n",
      "[CV] n_estimators=300, min_child_weight=5, max_depth=5, gamma=0.1, eta=0.05, colsample_bytree=0.7 \n",
      "[CV]  n_estimators=300, min_child_weight=5, max_depth=5, gamma=0.1, eta=0.05, colsample_bytree=0.7, score=0.697, total=  18.3s\n",
      "[CV] n_estimators=300, min_child_weight=5, max_depth=5, gamma=0.1, eta=0.05, colsample_bytree=0.7 \n",
      "[CV]  n_estimators=300, min_child_weight=5, max_depth=5, gamma=0.1, eta=0.05, colsample_bytree=0.7, score=0.698, total=  18.2s\n",
      "[CV] n_estimators=500, min_child_weight=7, max_depth=15, gamma=0.1, eta=0.05, colsample_bytree=0.4 \n",
      "[CV]  n_estimators=500, min_child_weight=7, max_depth=15, gamma=0.1, eta=0.05, colsample_bytree=0.4, score=0.769, total=  35.7s\n",
      "[CV] n_estimators=500, min_child_weight=7, max_depth=15, gamma=0.1, eta=0.05, colsample_bytree=0.4 \n",
      "[CV]  n_estimators=500, min_child_weight=7, max_depth=15, gamma=0.1, eta=0.05, colsample_bytree=0.4, score=0.768, total=  36.5s\n",
      "[CV] n_estimators=500, min_child_weight=7, max_depth=15, gamma=0.1, eta=0.05, colsample_bytree=0.4 \n",
      "[CV]  n_estimators=500, min_child_weight=7, max_depth=15, gamma=0.1, eta=0.05, colsample_bytree=0.4, score=0.771, total=  36.1s\n",
      "[CV] n_estimators=500, min_child_weight=7, max_depth=15, gamma=0.4, eta=0.05, colsample_bytree=0.4 \n",
      "[CV]  n_estimators=500, min_child_weight=7, max_depth=15, gamma=0.4, eta=0.05, colsample_bytree=0.4, score=0.769, total=  35.1s\n",
      "[CV] n_estimators=500, min_child_weight=7, max_depth=15, gamma=0.4, eta=0.05, colsample_bytree=0.4 \n",
      "[CV]  n_estimators=500, min_child_weight=7, max_depth=15, gamma=0.4, eta=0.05, colsample_bytree=0.4, score=0.767, total=  35.8s\n",
      "[CV] n_estimators=500, min_child_weight=7, max_depth=15, gamma=0.4, eta=0.05, colsample_bytree=0.4 \n",
      "[CV]  n_estimators=500, min_child_weight=7, max_depth=15, gamma=0.4, eta=0.05, colsample_bytree=0.4, score=0.770, total=  35.4s\n",
      "[CV] n_estimators=300, min_child_weight=3, max_depth=12, gamma=0.0, eta=0.05, colsample_bytree=0.7 \n",
      "[CV]  n_estimators=300, min_child_weight=3, max_depth=12, gamma=0.0, eta=0.05, colsample_bytree=0.7, score=0.750, total=  33.7s\n",
      "[CV] n_estimators=300, min_child_weight=3, max_depth=12, gamma=0.0, eta=0.05, colsample_bytree=0.7 \n",
      "[CV]  n_estimators=300, min_child_weight=3, max_depth=12, gamma=0.0, eta=0.05, colsample_bytree=0.7, score=0.753, total=  34.4s\n",
      "[CV] n_estimators=300, min_child_weight=3, max_depth=12, gamma=0.0, eta=0.05, colsample_bytree=0.7 \n",
      "[CV]  n_estimators=300, min_child_weight=3, max_depth=12, gamma=0.0, eta=0.05, colsample_bytree=0.7, score=0.748, total=  34.4s\n",
      "[CV] n_estimators=500, min_child_weight=1, max_depth=5, gamma=0.4, eta=0.05, colsample_bytree=0.3 \n",
      "[CV]  n_estimators=500, min_child_weight=1, max_depth=5, gamma=0.4, eta=0.05, colsample_bytree=0.3, score=0.729, total=  23.5s\n",
      "[CV] n_estimators=500, min_child_weight=1, max_depth=5, gamma=0.4, eta=0.05, colsample_bytree=0.3 \n",
      "[CV]  n_estimators=500, min_child_weight=1, max_depth=5, gamma=0.4, eta=0.05, colsample_bytree=0.3, score=0.734, total=  23.6s\n",
      "[CV] n_estimators=500, min_child_weight=1, max_depth=5, gamma=0.4, eta=0.05, colsample_bytree=0.3 \n",
      "[CV]  n_estimators=500, min_child_weight=1, max_depth=5, gamma=0.4, eta=0.05, colsample_bytree=0.3, score=0.731, total=  23.5s\n",
      "[CV] n_estimators=300, min_child_weight=7, max_depth=4, gamma=0.0, eta=0.05, colsample_bytree=0.5 \n",
      "[CV]  n_estimators=300, min_child_weight=7, max_depth=4, gamma=0.0, eta=0.05, colsample_bytree=0.5, score=0.674, total=  16.3s\n",
      "[CV] n_estimators=300, min_child_weight=7, max_depth=4, gamma=0.0, eta=0.05, colsample_bytree=0.5 \n",
      "[CV]  n_estimators=300, min_child_weight=7, max_depth=4, gamma=0.0, eta=0.05, colsample_bytree=0.5, score=0.680, total=  16.3s\n",
      "[CV] n_estimators=300, min_child_weight=7, max_depth=4, gamma=0.0, eta=0.05, colsample_bytree=0.5 \n",
      "[CV]  n_estimators=300, min_child_weight=7, max_depth=4, gamma=0.0, eta=0.05, colsample_bytree=0.5, score=0.674, total=  16.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 13.2min finished\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier(tree_method='gpu_hist', gpu_id=0)\n",
    "\n",
    "parameters = {\"eta\" : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "     \"max_depth\" : [3, 4, 5, 6, 8, 10, 12, 15],\n",
    "     \"min_child_weight\" : [1, 3, 5, 7],\n",
    "     \"gamma\" : [0.0, 0.1, 0.2 , 0.3, 0.4],\n",
    "     \"colsample_bytree\" : [0.3, 0.4, 0.5 , 0.7],\n",
    "     \"n_estimators\": [1000, 2000, 3000]}\n",
    "\n",
    "\n",
    "xgb_rscv = RandomizedSearchCV(xgb_clf, param_distributions = parameters, scoring = \"f1\",\n",
    "                             cv = 4, verbose = 3, random_state = 12345, n_jobs = 1)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model_xgboost = xgb_rscv.fit(X_train, y_train)\n",
    "\n",
    "best_par = model_xgboost.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98     35806\n",
      "           1       0.84      0.72      0.78      4087\n",
      "\n",
      "    accuracy                           0.96     39893\n",
      "   macro avg       0.90      0.85      0.88     39893\n",
      "weighted avg       0.96      0.96      0.96     39893\n",
      "\n",
      "F1 = 0.7763590891141239\n"
     ]
    }
   ],
   "source": [
    "preds = model_xgboost.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, preds))\n",
    "print(\"F1 = {}\".format(f1_score(y_test, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На модели логистической регрессии удалось достичь значение метрики F1  0.760274860624919\n",
    "\n",
    "У модели случайного леса с 100 деревьями удалось достичь значение метрики F1 0.7546261089987326\n",
    "\n",
    "Модель градиентно бустинга достигла значения F1 0.7763590891141239"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель градиентного бустинга показала лучший результат метрики F1 на валидационной выборке, на тестовой выборке удалось достичь показателся F1: 0.7763590891141239\n",
    "\n",
    "Так же по сравнению со случайным лесом модель обучается быстрее и показывает лучший скор."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
